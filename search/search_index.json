{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Authors Biography #name print('Ainur Inas Annisa') #email print('ainur.inas.annisa@gmail.com') error = True questions = True if(error!=False or questions!=False): print('My email is available for discussions') else : print('Good luck :)')","title":"Home"},{"location":"#authors-biography","text":"#name print('Ainur Inas Annisa') #email print('ainur.inas.annisa@gmail.com') error = True questions = True if(error!=False or questions!=False): print('My email is available for discussions') else : print('Good luck :)')","title":"Authors Biography"},{"location":"about/","text":"Introduction Pagerank merupakan suatu algoritma yang bertujuan untuk menentukan kepopuleran salah satu situs. Pagerank merupakan salah satu layanan yang disediakan oleh Google. Pagerank disediakan oleh Mesin Pencari Google dan dibuat oleh salah satu ceo dan pendiri google yaitu Sergey Brin Dan Larry Page . Pagerank berfungsi menentukan situs web mana yang lebih penting atau populer. Semakin tinggi pagerank yang dimiliki sebuah halaman web maka semakin populer halaman web tersebut, karena dengan pagerank yang tinggi yang milikinya itu berarti bahwa situs web tersebut banyak ditautkan oleh situs web lain. Untuk mendapatkan situs yang populer dapat diketahui dengan mendapatkan jalur sitasi dari situs lain. Hal ini dapat dilakukan dengan salah satu cara yakni melakukan proses crawling pada situs dan mendapatkan data berupa link yang terdapat pada situs tersebut. Selanjutnya membuat jalur graf untuk mengetahui situs mana saja yang paling banyak disitasi. Untuk mengetahui populer tidaknya suatu situs dapat diurutkan berdasarkan nilai pagerank dengan inputan graf yang telah dibuat. Step 1. Crawling Apa itu web crawling ? Web Crawler merupakan suatu program atau Script otomatis yang relatif simpel, menggunakan sebuah metode tertentu untuk melakukan scan atau crawl pada halaman internet untuk mendapatkan indek dari data yang dicari. Nama lain dari Web Crawler adalah Web Spider , Web Robot , Bot , Crawl dan Automatic Indexer . Untuk penjelasan tentang web crawling dapat teman-teman baca pada postingan sebelumnya disini . Apa yang akan dilakukan ? Install library yang digunakan import requests from bs4 import BeautifulSoup BeautifulSoup digunakan untuk dapat mengakses data html dan xml, sedangkan requests digunakan untuk dapat mengakses halaman web. Menentukan root page Root page merupakan halaman web berupa berita yang akan menjadi root atau akar atau base link yang akan di crawling . Berita yang akan digunakan adalah https://www.suara.com/news/2019/05/22/190121/kapolres-jakpus-minta-polisi-bertahan-pendemo-22-mei-terus-lempar-batu Menentukan link yang akan dicrawl Untuk mendapatkan link yang berelasi dengan berita, content yang akan di crawling adalah pada bagian Berita Terkait . Sama seperti postingan sebelumnya, untuk mendapatkan tag html Berita Terkait yang dilakukan adalah melakukan inspect element . inspect element dapat dilakukan dengan cara : pada halaman berita, klik kanan >> inspect maka panel inspect element akan muncul seperti gambar dibawah. Atau menekan Ctrl+Shift+I . Selanjutnya cari tag html yang memuat link artikel pada Berita Terkait , pada kasus ini tag html yang memuat link tersebut terletak di <a href=\"https://www.suara.com/news/2019/05/23/131313/janes-sosok-inspiratif-di-balik-kerusuhan-22-mei-pungut-sampah-pendemo\" class=\"ellipsis3\">Janes, Sosok Inspiratif di Balik Kerusuhan 22 Mei, Pungut Sampah Pendemo</a> . Sehingga untuk melakukan crawling pada artikel tersebut, diperoleh code seperti berikut : req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_links = soup.find_all('a',{'class':'ellipsis3'}, href=True) Looping hingga melakukan crawling pada kedalaman tertentu Menginisialisasi kedalaman digunakan sebagai suatu kondisi agar looping berhenti, hal ini juga dapat digunakan untuk membatasi banyaknya link yang akan di crawling . Full Code import requests from bs4 import BeautifulSoup def getLinks(): url = \"https://www.suara.com/news/2019/05/22/190121/kapolres-jakpus-minta-polisi-bertahan-pendemo-22-mei-terus-lempar-batu\" listUrl.append(url) listUrl.append(\"depth\") count_depth = 3 depth = 1 nav = 0 cek = True while cek == True : if listUrl[nav] == \"depth\" : listUrl.append(\"depth\") depth+=1 nav+=1 if depth > count_depth: cek = False else : url = listUrl[nav] req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_links = soup.find_all('a',{'class':'ellipsis3'}, href=True) for link in news_links: listUrl.append(link['href']) nav+=1 if __name__== \"__main__\": listUrl = [] links = getLinks() Hasil Step 2. Graph Processing Apa itu graph ? Graph atau Graf adalah kumpulan noktah (simpul) di dalam bidang dua dimensi yang dihubungkan dengan sekumpulan garis (sisi). Graph dapat digunakan untuk merepresentasikan objek-objek diskrit dan hubungan antara objek-objek tersebut. Representasi visual dari graph adalah dengan menyatakan objek sebagai noktah, bulatan atau titik (Vertex), sedangkan hubungan antara objek dinyatakan dengan garis (Edge). G = (V, E) Dimana : G = Graph V = Simpul atau Vertex, atau Node, atau Titik E = Busur atau Edge, atau arc V adalah himpunan verteks dan E adalah himpunan sisi yang terdefinisi antara pasangan-pasangan verteks. Sebuah sisi antara verteks x dan y ditulis {x,y}. Suatu graph H = (V1, E1) disebut subgraph dari graph G jika V1 adalah himpunan bagian dari V dan E1 himpunan bagian dari E. Cara pendefinisian lain untuk graph adalah dengan menggunakan himpunan keterhubungan langsung Vx. Pada setiap verteks x terdefinisi Vx sebagai himpunan dari verteks-verteks yang adjacent dari x. Secara formal: Vx = {y | (x,y) -> E} Apa manfaat dari graph ? Tiap-tiap diagram memuat sekumpulan obyek (kotak, titik, dan lain-lain) beserta garis-garis yang menghubungkan obyek-obyek tersebut. Garis bisa berarah ataupun tidak berarah. Garis yang berarah biasanya digunakan untuk menyatakan hubungan yang mementingkan urutan antar objek-objek. Urut-urutan objek akan mempunyai arti yang lain jika arah garis diubah. Sebagai contoh adalah garis komando yang menghubungkan titik-titik struktur sebuah organisasi. Sebaliknya, garis yang tidak berarah digunakan untuk menyatakan hubungan antar objek-objek yang tidak mementingkan urutan. Sebagai contoh adalah garis untuk menyatakan jarak hubung 2 kota pada Gambar 2. Jarak dari kota A ke kota B sejauh 200 km akan sama dengan jarak dari kota B ke kota A. Apabila jarak 2 tempat tidak sama jika dibalik (misalnya karena harus melalui jalan memutar), maka garis yang digunakan haruslah garis yang berarah. Apa yang akan dilakukan ? Install library yang digunakan import matplotlib.pyplot as plt import networkx as nx matplotlib digunakan untuk dapat memvisualisasikan graph , sedangkan networkx digunakan untuk dapat mengakses atau membangun graph . Menentukan node dan edge Node merupakan kumpulan dari link yang telah diperoleh dari hasil crawling, sedangkan Edge merupakan hubungan antar link atau link yang saling berkaitan. Oleh karena itu, saat melakukan crawling alangkah lebih baiknya jika menginisialisasi Node dan Edge secara bersamaan. url = listUrl[nav] req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_links = soup.find_all('a',{'class':'ellipsis3'}, href=True) node_A = node.index(url) for link in news_links: listUrl.append(link['href']) if link['href'] not in node : node.append(link['href']) node_B = node.index(link['href']) value = (str(node_A),str(node_B)) edge.append(value) Saat mengunjungi sebuah artikel, link dari artikel tersebut akan di tambahkan ke dalam list node node.append(link['href']) , setelah itu link dari artikel node_A = node.index(url) dan link terkait node_B = node.index(link['href']) di inisialisasi dan ditambakan pada list edge, value = (str(node_A),str(node_B)) edge.append(value) Sehingga keseluruhan code untuk proses crawling dan inisialisasi Node dan Edge diperoleh : import my_function as func import requests from bs4 import BeautifulSoup def getLinks(): url = \"https://www.suara.com/news/2019/05/22/190121/kapolres-jakpus-minta-polisi-bertahan-pendemo-22-mei-terus-lempar-batu\" listUrl.append(url) listUrl.append(\"depth\") node.append(url) count_depth = 3 depth = 1 nav = 0 cek = True while cek == True : if listUrl[nav] == \"depth\" : listUrl.append(\"depth\") depth+=1 nav+=1 if depth > count_depth: cek = False else : url = listUrl[nav] req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_links = soup.find_all('a',{'class':'ellipsis3'}, href=True) node_A = node.index(url) for link in news_links: listUrl.append(link['href']) if link['href'] not in node : node.append(link['href']) node_B = node.index(link['href']) value = (str(node_A),str(node_B)) edge.append(value) nav+=1 if __name__== \"__main__\": listUrl = [] node = [] edge = [] links = getLinks() func.save_list(\"node\",node) func.save_list(\"edge\",edge) func.save_list(\"listUrl\",listUrl) Untuk import my_function as func digunakan untuk menyimpan hasil dari proses diatas. Function ini telah dibahas pada postingan sebelumnya, code nya juga tersedia pada postingan sebelumnya. Gambar di bawah merupakan himpunan dari Node atau link hasil crawling . Gambar di bawah merupakan Edge atau keterkaitan antar link dari hasil crawling . Node pada Edge dipresentasikan dengan indeks dari list Node , hal ini bertujuan sebagai pengganti label, sehingga diperoleh hasil seperti gambar dibawah ini : Membangun graph Setelah Node dan Edge sudah diinisialisasi, gunakan library networkx untuk membuat node dan edge pada graph . Kemudian gunakan nx.draw_circular untuk menvisualisasi kan. G=nx.DiGraph() # a list of nodes: pages = [] for i in range(0,len(node)): pages.append(str(i)) G.add_nodes_from(pages) G.add_edges_from(edge) nx.draw_circular(G,node_color='red', with_labels = True) Gunakan library matplotlib untuk menyimpan dan menampilkan hasil graph . nx.draw_circular(G,node_color='red', with_labels = True) plt.show() # display plt.savefig(\"path.png\") Full Code import matplotlib.pyplot as plt import networkx as nx import numpy as np import my_function as func if __name__== \"__main__\": node = func.load_list(\"node\") edge = func.load_list(\"edge\") G=nx.DiGraph() # a list of nodes: pages = [] for i in range(0,len(node)): pages.append(str(i)) G.add_nodes_from(pages) G.add_edges_from(edge) print(\"Nodes of graph: \") print(G.nodes()) print(\"Edges of graph: \") print(G.edges()) print(\"Number of outward links for each node:\") for page in pages: print([\"Page %s = %s\"% (page,str(len(G.out_edges(page))))]) # nx.draw(G, with_labels = True) # plt.show() # display nx.draw_circular(G,node_color='red', with_labels = True) plt.show() # display plt.savefig(\"path.png\") Hasil Step 3. Page Rank Apa itu page rank ? Kata Pagerank berasal dari salah satu istilah para pengguna blog di dalam sistem yang dibuat mesin pencari untuk menilai dan memberikan peringkat pagerank kepada suatu situs. Mesin pencari yang memberikan pagerank kepada suatu situs ini adalah Mesin Pencari Google. Pagerank juga merupakan suatu algoritma yang bertujuan untuk menentukan kepopuleran salah satu situs. Pagerank merupakan salah satu layanan yang disediakan oleh Google. Pagerank disediakan oleh Mesin Pencari Google dan dibuat oleh salah satu ceo dan pendiri google yaitu Sergey Brin Dan Larry Page . Apa manfaat dari page rank ? Fungsi utama dari pagerank yaitu menganalisis berbagai link yang masuk ( backlink ) kemudian akan dihitung berapa jumlah link yang masuk ( Inbound ) dan link yang keluar ( Outbound ) dari sebuah halaman web tersebut, yang kemudian akan menghasilkan pagerank yang kita dapatkan. Pagerank juga berfungsi menentukan situs web mana yang lebih penting atau populer mulai dari skala yang paling tinggi 10 hingga skala yang paling rendah yaitu 0. Semakin tinggi pagerank yang dimiliki sebuah halaman web maka semakin populer halaman web tersebut, karena dengan pagerank yang tinggi yang milikinya itu berarti bahwa situs web tersebut banyak ditautkan oleh situs web lain. Apa yang akan dilakukan ? Menentukan page rank PR = nx.pagerank(G) value = max(PR, key=PR.get) print(PR) print(\"The most important node is \" + value) print(\"The most important link is \" + node[int(value)]) Untuk menentukan page rank dapat menggunakan library yang tersedia yakni nx.pagerank(G) . Dimana secara default , pagerank(G, alpha=0.85, personalization=None, max_iter=100, tol=1e-06, nstart=None, weight='weight', dangling=None) G ( graph ) alpha \u2013 Damping parameter for PageRank, default=0.85. personalization \u2013 The \u201cpersonalization vector\u201d consisting of a dictionary with a key for every graph node and nonzero personalization value for each node. By default, a uniform distribution is used. max_iter \u2013 Maximum number of iterations in power method eigenvalue solver. tol \u2013 Error tolerance used to check convergence in power method solver. nstart \u2013 Starting value of PageRank iteration for each node. weight \u2013 Edge data key to use as weight. If None weights are set to 1. dangling \u2013 The outedges to be assigned to any \u201cdangling\u201d nodes, i.e., nodes without any outedges. The dict key is the node the outedge points to and the dict value is the weight of that outedge. By default, dangling nodes are given outedges according to the personalization vector (uniform if not specified). This must be selected to result in an irreducible transition matrix (see notes under google_matrix). It may be common to have the dangling dict to be the same as the personalization dict. Full Code import matplotlib.pyplot as plt import networkx as nx import numpy as np import my_function as func if __name__== \"__main__\": node = func.load_list(\"node\") edge = func.load_list(\"edge\") G=nx.DiGraph() # a list of nodes: pages = [] for i in range(0,len(node)): pages.append(str(i)) G.add_nodes_from(pages) G.add_edges_from(edge) print(\"Nodes of graph: \") print(G.nodes()) print(\"Edges of graph: \") print(G.edges()) print(\"Number of outward links for each node:\") for page in pages: print([\"Page %s = %s\"% (page,str(len(G.out_edges(page))))]) image = nx.draw_circular(G,node_color='red', with_labels = True) PR = nx.pagerank(G) value = max(PR, key=PR.get) print(PR) print(\"The most important node is \" + value) print(\"The most important link is \" + node[int(value)]) Hasil {'0': 0.010786569775923953, '1': 0.07877510607400716, '2': 0.08429369074060056, '3': 0.07809532275240762, '4': 0.0832487848846966, '5': 0.07877510607400716, '6': 0.09362252732892531, '7': 0.05898444024971937, '8': 0.04722108385186022, '9': 0.05449031715893964, '10': 0.059246340407318104, '11': 0.053614561859208744, '12': 0.04898793734885993, '13': 0.07513404487029707, '14': 0.030085143833367904, '15': 0.01869154414248863, '16': 0.01869154414248863, '17': 0.013627967252441656, '18': 0.013627967252441656} Diperoleh pagerank dari masing-masing link seperti diatas. Sehingga, yang memiliki nilai maksimum merupakan node yang paling penting. The most important node is 6 Dan node dengan label 6 merupakan link dari : https://www.suara.com/news/2019/05/22/200031/fadli-zon-orasi-di-aksi-22-mei-jangan-sampai-meninggal-dunia Conclusion Pada artikel kali ini, diperoleh nilai pagerank dari masing-masing link yang didapat pada proses crawling , nilai pagerank digunakan untuk mengetahui artikel mana yang paling banyak diakses. Full Code bisa didownload di sini disini . References http://skipperkongen.dk/2016/08/16/how-to-compute-the-pagerank-of-almost-anything/ https://networkx.github.io/documentation/networkx-1.7/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html http://t4urusboy08.blogspot.com/ http://ovieciinduts.blogspot.com/2012/01/teori-graf.html https://rahman371.wordpress.com/2014/10/10/apa-itu-pagerank/ https://www.caramanual.com/2017/01/apa-itu-pagerank-inilah-pengertian-dan.html","title":"Pagerank"},{"location":"about/#introduction","text":"Pagerank merupakan suatu algoritma yang bertujuan untuk menentukan kepopuleran salah satu situs. Pagerank merupakan salah satu layanan yang disediakan oleh Google. Pagerank disediakan oleh Mesin Pencari Google dan dibuat oleh salah satu ceo dan pendiri google yaitu Sergey Brin Dan Larry Page . Pagerank berfungsi menentukan situs web mana yang lebih penting atau populer. Semakin tinggi pagerank yang dimiliki sebuah halaman web maka semakin populer halaman web tersebut, karena dengan pagerank yang tinggi yang milikinya itu berarti bahwa situs web tersebut banyak ditautkan oleh situs web lain. Untuk mendapatkan situs yang populer dapat diketahui dengan mendapatkan jalur sitasi dari situs lain. Hal ini dapat dilakukan dengan salah satu cara yakni melakukan proses crawling pada situs dan mendapatkan data berupa link yang terdapat pada situs tersebut. Selanjutnya membuat jalur graf untuk mengetahui situs mana saja yang paling banyak disitasi. Untuk mengetahui populer tidaknya suatu situs dapat diurutkan berdasarkan nilai pagerank dengan inputan graf yang telah dibuat.","title":"Introduction"},{"location":"about/#step-1-crawling","text":"","title":"Step 1. Crawling"},{"location":"about/#apa-itu-web-crawling","text":"Web Crawler merupakan suatu program atau Script otomatis yang relatif simpel, menggunakan sebuah metode tertentu untuk melakukan scan atau crawl pada halaman internet untuk mendapatkan indek dari data yang dicari. Nama lain dari Web Crawler adalah Web Spider , Web Robot , Bot , Crawl dan Automatic Indexer . Untuk penjelasan tentang web crawling dapat teman-teman baca pada postingan sebelumnya disini .","title":"Apa itu web crawling ?"},{"location":"about/#apa-yang-akan-dilakukan","text":"","title":"Apa yang akan dilakukan ?"},{"location":"about/#install-library-yang-digunakan","text":"import requests from bs4 import BeautifulSoup BeautifulSoup digunakan untuk dapat mengakses data html dan xml, sedangkan requests digunakan untuk dapat mengakses halaman web.","title":"Install library yang digunakan"},{"location":"about/#menentukan-root-page","text":"Root page merupakan halaman web berupa berita yang akan menjadi root atau akar atau base link yang akan di crawling . Berita yang akan digunakan adalah https://www.suara.com/news/2019/05/22/190121/kapolres-jakpus-minta-polisi-bertahan-pendemo-22-mei-terus-lempar-batu","title":"Menentukan root page"},{"location":"about/#menentukan-link-yang-akan-dicrawl","text":"Untuk mendapatkan link yang berelasi dengan berita, content yang akan di crawling adalah pada bagian Berita Terkait . Sama seperti postingan sebelumnya, untuk mendapatkan tag html Berita Terkait yang dilakukan adalah melakukan inspect element . inspect element dapat dilakukan dengan cara : pada halaman berita, klik kanan >> inspect maka panel inspect element akan muncul seperti gambar dibawah. Atau menekan Ctrl+Shift+I . Selanjutnya cari tag html yang memuat link artikel pada Berita Terkait , pada kasus ini tag html yang memuat link tersebut terletak di <a href=\"https://www.suara.com/news/2019/05/23/131313/janes-sosok-inspiratif-di-balik-kerusuhan-22-mei-pungut-sampah-pendemo\" class=\"ellipsis3\">Janes, Sosok Inspiratif di Balik Kerusuhan 22 Mei, Pungut Sampah Pendemo</a> . Sehingga untuk melakukan crawling pada artikel tersebut, diperoleh code seperti berikut : req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_links = soup.find_all('a',{'class':'ellipsis3'}, href=True)","title":"Menentukan link yang akan dicrawl"},{"location":"about/#looping-hingga-melakukan-crawling-pada-kedalaman-tertentu","text":"Menginisialisasi kedalaman digunakan sebagai suatu kondisi agar looping berhenti, hal ini juga dapat digunakan untuk membatasi banyaknya link yang akan di crawling .","title":"Looping hingga melakukan crawling pada kedalaman tertentu"},{"location":"about/#full-code","text":"import requests from bs4 import BeautifulSoup def getLinks(): url = \"https://www.suara.com/news/2019/05/22/190121/kapolres-jakpus-minta-polisi-bertahan-pendemo-22-mei-terus-lempar-batu\" listUrl.append(url) listUrl.append(\"depth\") count_depth = 3 depth = 1 nav = 0 cek = True while cek == True : if listUrl[nav] == \"depth\" : listUrl.append(\"depth\") depth+=1 nav+=1 if depth > count_depth: cek = False else : url = listUrl[nav] req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_links = soup.find_all('a',{'class':'ellipsis3'}, href=True) for link in news_links: listUrl.append(link['href']) nav+=1 if __name__== \"__main__\": listUrl = [] links = getLinks()","title":"Full Code"},{"location":"about/#hasil","text":"","title":"Hasil"},{"location":"about/#step-2-graph-processing","text":"","title":"Step 2. Graph Processing"},{"location":"about/#apa-itu-graph","text":"Graph atau Graf adalah kumpulan noktah (simpul) di dalam bidang dua dimensi yang dihubungkan dengan sekumpulan garis (sisi). Graph dapat digunakan untuk merepresentasikan objek-objek diskrit dan hubungan antara objek-objek tersebut. Representasi visual dari graph adalah dengan menyatakan objek sebagai noktah, bulatan atau titik (Vertex), sedangkan hubungan antara objek dinyatakan dengan garis (Edge). G = (V, E) Dimana : G = Graph V = Simpul atau Vertex, atau Node, atau Titik E = Busur atau Edge, atau arc V adalah himpunan verteks dan E adalah himpunan sisi yang terdefinisi antara pasangan-pasangan verteks. Sebuah sisi antara verteks x dan y ditulis {x,y}. Suatu graph H = (V1, E1) disebut subgraph dari graph G jika V1 adalah himpunan bagian dari V dan E1 himpunan bagian dari E. Cara pendefinisian lain untuk graph adalah dengan menggunakan himpunan keterhubungan langsung Vx. Pada setiap verteks x terdefinisi Vx sebagai himpunan dari verteks-verteks yang adjacent dari x. Secara formal: Vx = {y | (x,y) -> E}","title":"Apa itu graph ?"},{"location":"about/#apa-manfaat-dari-graph","text":"Tiap-tiap diagram memuat sekumpulan obyek (kotak, titik, dan lain-lain) beserta garis-garis yang menghubungkan obyek-obyek tersebut. Garis bisa berarah ataupun tidak berarah. Garis yang berarah biasanya digunakan untuk menyatakan hubungan yang mementingkan urutan antar objek-objek. Urut-urutan objek akan mempunyai arti yang lain jika arah garis diubah. Sebagai contoh adalah garis komando yang menghubungkan titik-titik struktur sebuah organisasi. Sebaliknya, garis yang tidak berarah digunakan untuk menyatakan hubungan antar objek-objek yang tidak mementingkan urutan. Sebagai contoh adalah garis untuk menyatakan jarak hubung 2 kota pada Gambar 2. Jarak dari kota A ke kota B sejauh 200 km akan sama dengan jarak dari kota B ke kota A. Apabila jarak 2 tempat tidak sama jika dibalik (misalnya karena harus melalui jalan memutar), maka garis yang digunakan haruslah garis yang berarah.","title":"Apa manfaat dari graph ?"},{"location":"about/#apa-yang-akan-dilakukan_1","text":"","title":"Apa yang akan dilakukan ?"},{"location":"about/#install-library-yang-digunakan_1","text":"import matplotlib.pyplot as plt import networkx as nx matplotlib digunakan untuk dapat memvisualisasikan graph , sedangkan networkx digunakan untuk dapat mengakses atau membangun graph .","title":"Install library yang digunakan"},{"location":"about/#menentukan-node-dan-edge","text":"Node merupakan kumpulan dari link yang telah diperoleh dari hasil crawling, sedangkan Edge merupakan hubungan antar link atau link yang saling berkaitan. Oleh karena itu, saat melakukan crawling alangkah lebih baiknya jika menginisialisasi Node dan Edge secara bersamaan. url = listUrl[nav] req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_links = soup.find_all('a',{'class':'ellipsis3'}, href=True) node_A = node.index(url) for link in news_links: listUrl.append(link['href']) if link['href'] not in node : node.append(link['href']) node_B = node.index(link['href']) value = (str(node_A),str(node_B)) edge.append(value) Saat mengunjungi sebuah artikel, link dari artikel tersebut akan di tambahkan ke dalam list node node.append(link['href']) , setelah itu link dari artikel node_A = node.index(url) dan link terkait node_B = node.index(link['href']) di inisialisasi dan ditambakan pada list edge, value = (str(node_A),str(node_B)) edge.append(value) Sehingga keseluruhan code untuk proses crawling dan inisialisasi Node dan Edge diperoleh : import my_function as func import requests from bs4 import BeautifulSoup def getLinks(): url = \"https://www.suara.com/news/2019/05/22/190121/kapolres-jakpus-minta-polisi-bertahan-pendemo-22-mei-terus-lempar-batu\" listUrl.append(url) listUrl.append(\"depth\") node.append(url) count_depth = 3 depth = 1 nav = 0 cek = True while cek == True : if listUrl[nav] == \"depth\" : listUrl.append(\"depth\") depth+=1 nav+=1 if depth > count_depth: cek = False else : url = listUrl[nav] req = requests.get(url) soup = BeautifulSoup(req.text, 'html.parser') news_links = soup.find_all('a',{'class':'ellipsis3'}, href=True) node_A = node.index(url) for link in news_links: listUrl.append(link['href']) if link['href'] not in node : node.append(link['href']) node_B = node.index(link['href']) value = (str(node_A),str(node_B)) edge.append(value) nav+=1 if __name__== \"__main__\": listUrl = [] node = [] edge = [] links = getLinks() func.save_list(\"node\",node) func.save_list(\"edge\",edge) func.save_list(\"listUrl\",listUrl) Untuk import my_function as func digunakan untuk menyimpan hasil dari proses diatas. Function ini telah dibahas pada postingan sebelumnya, code nya juga tersedia pada postingan sebelumnya. Gambar di bawah merupakan himpunan dari Node atau link hasil crawling . Gambar di bawah merupakan Edge atau keterkaitan antar link dari hasil crawling . Node pada Edge dipresentasikan dengan indeks dari list Node , hal ini bertujuan sebagai pengganti label, sehingga diperoleh hasil seperti gambar dibawah ini :","title":"Menentukan node dan edge"},{"location":"about/#membangun-graph","text":"Setelah Node dan Edge sudah diinisialisasi, gunakan library networkx untuk membuat node dan edge pada graph . Kemudian gunakan nx.draw_circular untuk menvisualisasi kan. G=nx.DiGraph() # a list of nodes: pages = [] for i in range(0,len(node)): pages.append(str(i)) G.add_nodes_from(pages) G.add_edges_from(edge) nx.draw_circular(G,node_color='red', with_labels = True) Gunakan library matplotlib untuk menyimpan dan menampilkan hasil graph . nx.draw_circular(G,node_color='red', with_labels = True) plt.show() # display plt.savefig(\"path.png\")","title":"Membangun graph"},{"location":"about/#full-code_1","text":"import matplotlib.pyplot as plt import networkx as nx import numpy as np import my_function as func if __name__== \"__main__\": node = func.load_list(\"node\") edge = func.load_list(\"edge\") G=nx.DiGraph() # a list of nodes: pages = [] for i in range(0,len(node)): pages.append(str(i)) G.add_nodes_from(pages) G.add_edges_from(edge) print(\"Nodes of graph: \") print(G.nodes()) print(\"Edges of graph: \") print(G.edges()) print(\"Number of outward links for each node:\") for page in pages: print([\"Page %s = %s\"% (page,str(len(G.out_edges(page))))]) # nx.draw(G, with_labels = True) # plt.show() # display nx.draw_circular(G,node_color='red', with_labels = True) plt.show() # display plt.savefig(\"path.png\")","title":"Full Code"},{"location":"about/#hasil_1","text":"","title":"Hasil"},{"location":"about/#step-3-page-rank","text":"","title":"Step 3. Page Rank"},{"location":"about/#apa-itu-page-rank","text":"Kata Pagerank berasal dari salah satu istilah para pengguna blog di dalam sistem yang dibuat mesin pencari untuk menilai dan memberikan peringkat pagerank kepada suatu situs. Mesin pencari yang memberikan pagerank kepada suatu situs ini adalah Mesin Pencari Google. Pagerank juga merupakan suatu algoritma yang bertujuan untuk menentukan kepopuleran salah satu situs. Pagerank merupakan salah satu layanan yang disediakan oleh Google. Pagerank disediakan oleh Mesin Pencari Google dan dibuat oleh salah satu ceo dan pendiri google yaitu Sergey Brin Dan Larry Page .","title":"Apa itu page rank ?"},{"location":"about/#apa-manfaat-dari-page-rank","text":"Fungsi utama dari pagerank yaitu menganalisis berbagai link yang masuk ( backlink ) kemudian akan dihitung berapa jumlah link yang masuk ( Inbound ) dan link yang keluar ( Outbound ) dari sebuah halaman web tersebut, yang kemudian akan menghasilkan pagerank yang kita dapatkan. Pagerank juga berfungsi menentukan situs web mana yang lebih penting atau populer mulai dari skala yang paling tinggi 10 hingga skala yang paling rendah yaitu 0. Semakin tinggi pagerank yang dimiliki sebuah halaman web maka semakin populer halaman web tersebut, karena dengan pagerank yang tinggi yang milikinya itu berarti bahwa situs web tersebut banyak ditautkan oleh situs web lain.","title":"Apa manfaat dari page rank ?"},{"location":"about/#apa-yang-akan-dilakukan_2","text":"","title":"Apa yang akan dilakukan ?"},{"location":"about/#menentukan-page-rank","text":"PR = nx.pagerank(G) value = max(PR, key=PR.get) print(PR) print(\"The most important node is \" + value) print(\"The most important link is \" + node[int(value)]) Untuk menentukan page rank dapat menggunakan library yang tersedia yakni nx.pagerank(G) . Dimana secara default , pagerank(G, alpha=0.85, personalization=None, max_iter=100, tol=1e-06, nstart=None, weight='weight', dangling=None) G ( graph ) alpha \u2013 Damping parameter for PageRank, default=0.85. personalization \u2013 The \u201cpersonalization vector\u201d consisting of a dictionary with a key for every graph node and nonzero personalization value for each node. By default, a uniform distribution is used. max_iter \u2013 Maximum number of iterations in power method eigenvalue solver. tol \u2013 Error tolerance used to check convergence in power method solver. nstart \u2013 Starting value of PageRank iteration for each node. weight \u2013 Edge data key to use as weight. If None weights are set to 1. dangling \u2013 The outedges to be assigned to any \u201cdangling\u201d nodes, i.e., nodes without any outedges. The dict key is the node the outedge points to and the dict value is the weight of that outedge. By default, dangling nodes are given outedges according to the personalization vector (uniform if not specified). This must be selected to result in an irreducible transition matrix (see notes under google_matrix). It may be common to have the dangling dict to be the same as the personalization dict.","title":"Menentukan page rank"},{"location":"about/#full-code_2","text":"import matplotlib.pyplot as plt import networkx as nx import numpy as np import my_function as func if __name__== \"__main__\": node = func.load_list(\"node\") edge = func.load_list(\"edge\") G=nx.DiGraph() # a list of nodes: pages = [] for i in range(0,len(node)): pages.append(str(i)) G.add_nodes_from(pages) G.add_edges_from(edge) print(\"Nodes of graph: \") print(G.nodes()) print(\"Edges of graph: \") print(G.edges()) print(\"Number of outward links for each node:\") for page in pages: print([\"Page %s = %s\"% (page,str(len(G.out_edges(page))))]) image = nx.draw_circular(G,node_color='red', with_labels = True) PR = nx.pagerank(G) value = max(PR, key=PR.get) print(PR) print(\"The most important node is \" + value) print(\"The most important link is \" + node[int(value)])","title":"Full Code"},{"location":"about/#hasil_2","text":"{'0': 0.010786569775923953, '1': 0.07877510607400716, '2': 0.08429369074060056, '3': 0.07809532275240762, '4': 0.0832487848846966, '5': 0.07877510607400716, '6': 0.09362252732892531, '7': 0.05898444024971937, '8': 0.04722108385186022, '9': 0.05449031715893964, '10': 0.059246340407318104, '11': 0.053614561859208744, '12': 0.04898793734885993, '13': 0.07513404487029707, '14': 0.030085143833367904, '15': 0.01869154414248863, '16': 0.01869154414248863, '17': 0.013627967252441656, '18': 0.013627967252441656} Diperoleh pagerank dari masing-masing link seperti diatas. Sehingga, yang memiliki nilai maksimum merupakan node yang paling penting. The most important node is 6 Dan node dengan label 6 merupakan link dari : https://www.suara.com/news/2019/05/22/200031/fadli-zon-orasi-di-aksi-22-mei-jangan-sampai-meninggal-dunia","title":"Hasil"},{"location":"about/#conclusion","text":"Pada artikel kali ini, diperoleh nilai pagerank dari masing-masing link yang didapat pada proses crawling , nilai pagerank digunakan untuk mengetahui artikel mana yang paling banyak diakses. Full Code bisa didownload di sini disini .","title":"Conclusion"},{"location":"about/#references","text":"http://skipperkongen.dk/2016/08/16/how-to-compute-the-pagerank-of-almost-anything/ https://networkx.github.io/documentation/networkx-1.7/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html http://t4urusboy08.blogspot.com/ http://ovieciinduts.blogspot.com/2012/01/teori-graf.html https://rahman371.wordpress.com/2014/10/10/apa-itu-pagerank/ https://www.caramanual.com/2017/01/apa-itu-pagerank-inilah-pengertian-dan.html","title":"References"},{"location":"clustering/","text":"Introduction Menurut bahasa, data merupakan bentuk jamak dari kata datum (bahasa latin) yang berarti sesuatu yang diberikan . Menurut istilah, pengertian data adalah kumpulan informasi atau keterangan-keterangan yang diperoleh dari pengamatan, informasi itu bisa berupa angka, lambang atau sifat. Dalam kehidupan sehari-hari data berarti suatu pernyataan yang diterima secara apa adanya. Artinya data yang diperoleh dari berbagai sumbernya masih menjadi sebuah anggapan atau fakta karena memang belum diolah lebih lanjut. Setelah diolah melaui suatu penelitian atau percobaan maka data dapat berubah menjadi bentuk yang lebih kompleks misal database, informasi atau bahkan solusi pada masalah tertentu. Data yang telah diolah akan berubah menjadi sebuah informasi yang dapat digunakan untuk menambah pengetahuan bagi yang menerimanya. Maka dalam hal ini data dapat dianggap sebagai obyek dan informasi adalah suatu subyek yang bermanfaat bagi penerimanya. Informasi juga bisa disebut sebagai hasil pengolahan ataupun pemrosesan data. Oleh karena itu artikel ini akan membahas tutorial untuk mengolah data yang diperoleh dari crawling web sehingga diperoleh suatu informasi. Data yang didapat dari proses crawl akan diolah menggunakan metode Fuzzy C-Means untuk melakukan cluster pada data dan Silhouette Coefficient digunakan untuk menguji kualitas cluster yang diperoleh. Hasil akhir yang ingin dicapai pada artikel ini adalah membandingkan jumlah cluster sehingga mendapatkan jumlah cluster yang efiseien dan berkualitas berdasarkan perolehan skor dari Silhouette Coefficient . Selain itu pada tutorial ini juga terdapat proses Feature Selection untuk mengurangi jumlah fitur agar tidak memakan waktu komputasi yang banyak. Algoritma yang digunakan pada proses ini adalah Drop High Correlation , algortima ini digunakan untuk menghindari membuang fitur yang memiliki informasi yang diperlukan karena algoritma ini melakukan proses korelasi matrik untuk mengetahui fitur mana yang tidak memiliki korelasi dengan fitur yang lainnya. Sehingga algoritma ini dapat mencegah membuang fitur yang penting. Step 1. Installing Dalam tutorial ini semua proses akan dibangun menggunakan bahasa pemograman Python . Alasan mengapa author menggunakan bahasa pemograman ini karena Python merupakan bahasa pemograman yang populer akhir-akhir ini, selain itu Python menyediakan banyak library yang mudah digunakan dalam membangun suatu sistem. Adapun library yang digunakan akan dijelaskan pada setiap stepnya. Panduan instalasi python dapat teman-teman baca disini . Panduan instalasi library atau package dapat teman-teman baca disini . Step 2. Crawling Apa itu Web Crawler ? Web Crawler merupakan suatu program atau Script otomatis yang relatif simpel, menggunakan sebuah metode tertentu untuk melakukan scan atau crawl pada halaman internet untuk mendapatkan indek dari data yang dicari. Nama lain dari Web Crawler adalah Web Spider , Web Robot , Bot , Crawl dan Automatic Indexer . Umumnya Web Crawler dapat digunakan berkaitan dengan Search Engine , yakni mengumpulkan informasi mengenai apa yang ada pada halaman-halaman web publik. Bagaimana cara kerja Web Crawler ? Ketika Web Crawler suatu Search Engine mengunjungi halaman web, Web Crawler akan membaca teks , hyperlink dan macam-macam tag konten yang digunakan dalam situs misalnya meta tag yang berisi banyak keyword. Data tersebut kemudian akan dimasukkan ke dalam database atau tempat penyimpanan. Apa yang akan dilakukan ? Install library yang digunakan from bs4 import BeautifulSoup import requests BeautifulSoup digunakan untuk dapat mengakses data html dan xml, sedangkan requests digunakan untuk dapat mengakses halaman web. Menentukan halaman web yang akan di crawl Pada artikel ini halaman web yang akan di crawl datanya adalah https://www.suara.com/indeks/terkini/all/2019 . Halaman web ini memuat artikel atau berita https://www.suara.com yang diupload pada tahun 2019 saja. Menentukan informasi apa yang akan di simpan Setelah menentukan halaman web yang akan di crawl, selanjutnya berdasarkan informasi yang ada di halaman web diatas, data artikel atau berita yang akan di simpan pada tutorial ini berupa URL, judul dan isi berita . Proses Crawling >> Membuat requests pada halaman web yang dituju req = requests.get('https://www.suara.com/indeks/terkini/all/2019') >> Menentukan berita yang akan diambil dari halaman web Berita yang akan kita ambil adalah yang terletak di panel sebelah kiri, berita ini merupakan berita yang diupload pada tahun 2019. Sedangkan berita pada panel kanan merupakan headline setiap minggunya. Untuk dapat mengakses tag html dari list berita tersebut, dapat dilakukan dengan inspect element , yakni dengan cara : klik kanan pada halaman web pilih inspect maka akan muncul panel inspect di jendela sebelah kanan Selanjutnya pada panel inspect , cari tag html yang memuat list berita pada halaman web. Biasanya list berita memiliki nama class yang sama pada setiap berita. Pada halaman web diatas contohnya adalah class item-outer . news_links = soup.find_all(\"li\",{'class':'item-outer'}) >> Mendapatkan judul dan alamat web dari isi artikel Setelah tag html list berita telah diperoleh, selanjutnya adalah mencari tag html yang memuat judul dan link dari judul tersebut. Hal ini dilakukan agar menelusuri isi dari setiap list berita yang terdapat pada halaman tersebut. Biasanya tag html yang memuat link terdapat attribut href . Dari list berita tersebut, jika ditelusuri lebih mendalam tag html yang memuat link berita memiliki class ellipsis2 . for idx,news in enumerate(news_links): title_news= news.find('a',{'class':'ellipsis2'}).text url_news = news.find('a',{'class':'ellipsis2'}).get('href') Dilakukan perulangan untuk membaca semua link yang terdapat pada list berita. >> Membuat requests pada setiap berita req_news = requests.get(url_news) soup_news = BeautifulSoup(req_news.text, \"lxml\") Setelah mendapatkan link setiap berita, sama seperti langkah sebelumnya hal yang dilakukan adalah melakukan requests pada setiap berita. >> Mendapatkan isi artikel pada berita Sama seperti langkah sebelumnya, yang harus dilakukan selanjutnya adalah melakukan Inspect pada salah satu berita kemudiah mancari tag html yang mengandung isi dari berita tersebut. news_content = soup_news.find(\"div\",{'class':'content-article'}) p = news_content.find_all('p') content = ' '.join(item .text for item in p) news_content = content.encode('utf8','replace') Setelah mendapatkan isi dari suatu berita yang terdapat pada tag p selanjutnya menggabungkan isi semua tag tersbut menjadi sebuah string. Note : program diatas tidak dapat digunakan pada halaman web yang lain. Alasannya karena setiap web memiliki tag html yang berbeda. Jadi jika ingin melakukan crawling pada halaman web yang lain, teman-teman harus menyesuaikan dengan tag htmlnya dengan melakukan inspect yang telah dicontohkan diatas. Menyimpan data dalam bentuk excel import openpyxl def save_content(data): woorkbook = create_excel() woorkbook.create_sheet('crawl_data') sheet1 = woorkbook['crawl_data'] i = 0 content = [] for i in range(len(data)): for j in range(len(data[0])): sheet1.cell(row=i+1, column=j+1).value = data[i][j] if(j==1): content.append(data[i][j]) woorkbook.save('excel_logs/'+'crawl_data'+'.xlsx') Libary yang digunakan untuk mengolah data menggunakan excel adalah openpyxl . Library ini sangat mudah untuk mengakses data pada excel, setelah melakukan inisialisasi woorkbook = create_excel() dan mengakses sheet yang akan digunakan sheet1 = woorkbook['crawl_data'] , data dapat disimpan berdasarkan baris dan kolom pada excel sheet1.cell(row=i+1, column=j+1).value = data[i][j] . Setelah semua baris dan kolom sudah diberikan nilai maka selanjutnya adalah menyimpan file excel dengan woorkbook.save('nama.xlsx') . Menyimpan data dalam bentuk list import pickle def save_list(name,l): with open(\"data_logs/\"+name+\".txt\", \"wb\") as fp: #Pickling pickle.dump(l, fp) return print(\"Success to save....\") Data disimpan dalam bentuk list agar diproses selanjutnya data dapat digunakan tanpa melakukan crawling dari awal. Library yang digunakan adalah pickle . Full Code def crawl_web(url): new_url = url i = '?page=' page = 1 loop = True data = [] while loop == True : #find article link req = requests.get(new_url) soup = BeautifulSoup(req.text, \"lxml\") news_links = soup.find_all(\"li\",{'class':'item-outer'}) #check content available if not news_links : loop = False #looping through article link for idx,news in enumerate(news_links): try : #find news title title_news= news.find('a',{'class':'ellipsis2'}).text #find urll news url_news = news.find('a',{'class':'ellipsis2'}).get('href') #find news content in url req_news = requests.get(url_news) soup_news = BeautifulSoup(req_news.text, \"lxml\") #find news content news_content = soup_news.find(\"div\",{'class':'content-article'}) #find paragraph in news content p = news_content.find_all('p') content = ' '.join(item .text for item in p) news_content = content.encode('utf8','replace') tempt = [] tempt.append(title_news) tempt.append(news_content) tempt.append(url_news) data.append(tempt) except: pass print(page) #next pagination if(page>50): loop = False page+=1 new_url = url+i+str(page) return data Cara akses crawl = crawl_web('https://www.suara.com/indeks/terkini/all/2019') save_content(crawl) save_list(\"crawl\",crawl) List akan disimpan dengan nama crawl di directory data_log sedangkan file excel akan disimpan di directory excel_log . Sehingga sebelum code dijalankan, buatlah kedua folder tersebut terlebih dahulu pada directory yang sama dengan file .py Notes : dalam beberapa kasus akan muncul pesan error lxml saat code dijalankan, jika demikian yang harus dilakukan adalah menginstall library lxml. Hasil Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini . Step 3. Preprocessing Apa itu Text Preprocessing ? Berdasarkan ketidak teraturan struktur data teks, maka proses sistem temu kembali informasi ataupun text mining memerlukan beberapa tahap awal yang pada intinya adalah mempersiapkan agar teks dapat diubah menjadi lebih terstruktur. Salah satu implementasi dari text mining adalah tahap Text Preprocessing. Tahap Text Preprocessing adalah tahapan dimana aplikasi melakukan seleksi data yang akan diproses pada setiap dokumen. Proses preprocessing ini meliputi (1) case folding, (2) tokenizing, (3) filtering, dan (4) stemming. Apa yang akan dilakukan ? Install library yang digunakan import re from nltk.tokenize import word_tokenize from nltk.corpus import stopwords from Sastrawi.Stemmer.StemmerFactory import StemmerFactory Library yang digunakan pada text preprocessing ialah re digunakan untuk symbol removing , nltk digunakan untuk tokenizing dan stopword, Sastrawi digunakan untuk stemming. Symbol Removing def symbol_remover(data): new_data = [] for i in range(len(data)): new_data.append(re.sub(r'[^\\w]', ' ', data[i].replace(\"b suara com\", \"\"))) return new_data Case Folding def lowercase(data): new_data = [] for i in range(len(data)): tempt = str(data[i]).lower() new_data.append(tempt.replace('b\"suara.com - ',\"\")) return new_data Dalam sebuah dokumen penggunan huruf kapital atau sejenisnya terkadang tidak mempunyai kesamaan, hal ini bisa dikarenakan kesalahan penulisan. Dalam text preprocessing proses case folding bertujuan untuk mengubah semua huruf dalam sebuah dokumen teks menjadi huruf kecil (lowercase). Pada data yang didapatkan saat proses crawling , terdapat sebuah kalimat b\"suara.com diawal paragraf pada semua dokumen. Pada program diatas kalimat tersebut di hapus untuk mengurangi banyaknya fitur setiap berita. Tokenizing def tokenisasi(data): new_data = [] for i in range(len(data)): new_data.append(word_tokenize(data[i])) return new_data Dokumen teks terdiri dari sekumpulan kalimat, proses tokenisasi memecah dokumen tersebut menjadi bagian-bagian kata yang disebut token. Contohnya kalimat \"saya menyapu halaman di depan rumah\" setelah ditokenisasi menjadi sebuah list [\"saya\", \"menyapu\", \"halaman\", \"di\", \"depan\", \"rumah\"] . Stopword def stopword_s(data): new_data = [] stop_words = set(stopwords.words('indonesian')) for i in range(len(data)): tempt = [] for word in data[i]: if word not in stop_words: tempt.append(word) new_data.append(tempt) return new_data Selanjutnya ialah mengambil kata-kata yang dianggap penting dari hasil tokenization atau membuang kata-kata yang dianggap tidak terlalu mempunyai arti penting dalam proses text mining. Contohnya data [\"saya\", \"menyapu\", \"halaman\", \"di\", \"depan\", \"rumah\"] menjadi [\"saya\", \"menyapu\", \"halaman\", \"depan\", \"rumah\"] . Stemming def stemming(data): factory = StemmerFactory() stemmer = factory.create_stemmer() new_data = [] for i in range(len(data)): tempt = \"\" for word in data[i]: tempt = tempt + \" \" + word new_word = stemmer.stem(tempt) new_data.append(new_word.split()) return new_data Stemming bertujuan untuk mentransformasikan kata menjadi kata dasarnya (root word) dengan menghilangkan semua imbuhan kata. Contohnya data [\"saya\", \"sapu\", \"halaman\", \"depan\", \"rumah\"] menjadi [\"saya\", \"sapu\", \"halaman\", \"depan\", \"rumah\"] . Cara akses Sebelum melakukan proses diatas, terlebih dahulu membaca data hasil crawling yang telah disimpan dalam bentuk list dengan cara dibawah ini. def load_list(name): with open(\"data_logs/\"+name+\".txt\", \"rb\") as fp: # Unpickling b = pickle.load(fp) print(\"Success to load.....\") return b Selanjutnya, pergunakan semua function pada step ini secara bersama. print(\"Load data......\") data = load_list(\"data\") print(\"Lowercase......\") lowercase = lowercase(data) print(\"Remove symbol......\") symbol_remover = symbol_remover(lowercase) print(\"Tokenisasi......\") tokenisasi = tokenisasi(symbol_remover) print(\"Stopword......\") stopword_s = stopword_s(tokenisasi) print(\"Stemming......\") stemming = stemming(stopword_s) Pada hasil akhir dari tutorial ini, kita akan membandingkan uni-gram dan bi-gram. Uni-gram yakni hasil token merupakan tiap satu suku kata, sedangkan bi-gram hasil token menjadi 2 kata. Contoh bi-gram [\"saya sapu\", \"sapu halaman\", \"halaman depan\", \"depan rumah\"] . Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini . Step 4. Fiture Selection Apa itu Fiture Selection ? Pemilihan istilah untuk dijadikan indeks merupakan isu yang penting dalam sistem temu-kembali informasi. Selanjunya proses pemilihan istilah ini disebut dengan seleksi fitur (feature selection). Mengapa Fiture Selection penting ? Fitur seleksi dapat menyebabkan berkurangnya ukuran indeks sehingga proses retrieval suatu dokumen menjadi lebih cepat sebab jumlah indeks yang dicari menjadi lebih sedikit. Tugas utama seleksi fitur adalah menentukan istilah-istilah yang layak dijadikan term index atau dengan kata lain membuang (menghilangkan) istilah-istilah yang tidak mungkin dijadikan indeks. Apa itu Drop High Correlation ? Korelasi adalah istilah statistik yang dalam penggunaan umum mengacu pada seberapa dekat dua variabel untuk memiliki hubungan linier satu sama lain. Fitur dengan korelasi tinggi lebih linear dan karenanya memiliki efek yang hampir sama pada variabel dependen. Jadi, ketika dua fitur memiliki korelasi tinggi, kita dapat menjatuhkan ( drop ) salah satu dari dua fitur tersebut. Penghapusan fitur yang berbeda dari dataset akan memiliki efek yang berbeda pada nilai p untuk dataset. Kami dapat menghapus fitur yang berbeda dan mengukur nilai p di setiap kasus. Nilai-p yang diukur ini dapat digunakan untuk memutuskan apakah akan mempertahankan fitur atau tidak. Untuk informasi lebih lanjut mengenai perhitungan matrik korelasi bisa teman-teman baca disini . Apa yang akan dilakukan ? Install library yang digunakan import pandas as pd import numpy as np Library yang digunakan pada seleksi fitur ialah pandas digunakan untuk membuat data menjadi data frame dan numpy digunakan untuk menjadikan data menjadi matrix dan mengolahnya. Proses Fiture Selection >> Mengubah data menjadi dataframe Data yang berbentuk array dirubah menjadi dataframe , yakni struktur data tabel dua dimensi yang memiliki label. Hal ini akan memudahkan dalam operasi baris dan kolom. df = pd.DataFrame(X) >> Membuat korelasi matriks Dari matriks korelasi ini dapat dilihat bahwa pada bagian diagonal, nilainya pasti = 1 sebab variabel tersebut berkorelasi sempurna positif dengan dirinya sendiri. Nilai korelasi antara pasangan variabel pada set data1 dan data2, menunjukkan nilai dalam range -1 dan 1. Ini disebabkan oleh standarisasi menggunakan standar deviasi sebagai pembagi pada saat perhitungan hasil korelasinya. corr_matrix = df.corr().abs() >> Menentukan fitur yang akan dihapus Setelah mendapatkan matrik kolerasi, selanjutnya menentukan kolom yang memiliki nilai lebih dari 0.95 . Nilai ini fleksibel, bergantung pada batas nilai fitur yang dianggap penting. to_drop = [column for column in upper.columns if any(upper[column] > 0.95)] >> Menghapus fitur Proses selanjutnya ialah menghapus fitur yang dianggap tidak penting berdasarkan nilai batas yang ditentukan pada proses sebelumnya. new_data = df.drop(df.columns[to_drop], axis=1) Full Code def drop_highly_correlation(X,Y): # Convert feature matrix into DataFrame df = pd.DataFrame(X) # Create correlation matrix corr_matrix = df.corr().abs() # Select upper triangle of correlation matrix upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)) # Find index of feature columns with correlation greater than 0.95 to_drop = [column for column in upper.columns if any(upper[column] > 0.95)] # Drop features new_data = df.drop(df.columns[to_drop], axis=1) header = list(new_data.columns.values) new_collecting_fiture = [] for i in header : new_collecting_fiture.append(Y[i]) return new_data,new_collecting_fiture Cara akses collecting_fiture merupakan label dari setiap kolom atau list yang terdiri dari semua kata yang terdapat pada semua berita dimana tidak ada kata yang sama pada list. Panjang collecting_fiture sama dengan panjang kolom vsm . Sedangkan vsm adalah banyaknya kemunculan kata yang ada di collecting_fiture pada setiap berita. Sehingga diperoleh matrik 2-dimensi. def vsm(data,term): for i in range(len(data)): tempt = [] for word in term: tempt.append(data[i].count(word)) new_data.append(tempt) return new_data Sehingga vsm dan collecting_fiture digunakan sebagai input drop_highly_correlation . #Uni Gram vsm = load_list(\"vsm\") collecting_fiture = load_list(\"collecting_fiture\") new_vsm,new_collecting_fiture = drop_highly_correlation(vsm,collecting_fiture) #Bi Gram vsm_n_gram = load_list(\"vsm_2\") collecting_fiture_n_gram = load_list(\"collecting_fiture_2\") new_vsm_n_gram,new_collecting_fiture_n_gram = drop_highly_correlation(vsm_n_gram,collecting_fiture_n_gram) Hasil Hasil dari seleksi fitur ini adalah jumlah fitur atau atribut atau kolom pada data, baik uni-gram dan bi-gram akan berkurang. Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini . Step 5. TF-IDF Apa itu TF-IDF ? Metode TF-IDF merupakan metode untuk menghitung bobot setiap kata yang paling umum digunakan pada information retrieval. Metode ini juga terkenal efisien, mudah dan memiliki hasil yang akurat. Metode ini akan menghitung nilai Term Frequency (TF) dan Inverse Document Frequency (IDF) pada setiap token (kata) di setiap dokumen dalam korpus.. Apa yang akan dilakukan ? Install library yang digunakan import math Library yang digunakan ialah math yang digunakan untuk melakukan proses perhitungan. TF TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar. def tf(data,term): new_data=[] for i in range(len(data)): tempt = [] for word in term: tempt.append(data[i].count(word)) new_data.append(tempt) return new_data IDF IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah term dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung term yang dimaksud, maka nilai IDF semakin besar. def idf(data): new_data = [] for i in range(len(data[0])): count = 0 for j in range(len(data)): count+= int(data[j][i]) new_data.append(math.log10(len(data)/count)) return new_data TF-IDF def tf_idf(tf,idf): new_data = [] for i in range(len(tf)): tempt = [] for j in range(len(tf[i])): tempt.append(int(tf[i][j])*int(idf[j])) new_data.append(tempt) return new_data Cara akses #Uni Gram stemming = load_list(\"stemming\") collecting_fiture = load_list(\"new_collecting_fiture\") print(\"Load Tf....\") tf_a = tf(stemming,collecting_fiture) print(\"Load idf...\") idf_a = idf(tf_a) print(\"Load tf_idf...\") tf_idf_a = tf_idf(tf_a,idf_a,collecting_fiture) #Bi Gram n_gram = load_list(\"token_2\") collecting_fiture_n_gram = load_list(\"new_collecting_fiture_2\") print(\"Load Tf_n_gram....\") tf_n_gram = tf(n_gram,collecting_fiture_n_gram) print(\"Load idf_n_gram...\") idf_n_gram = idf(tf_n_gram) print(\"Load tf_idf_n_gram...\") tf_idf_n_gram = tf_idf(tf_n_gram,idf_n_gram,collecting_fiture_n_gram) Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini . Step 6. Clustering Apa itu Clustering ? Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Apa itu Fuzzy C-Means ? Fuzzy C-Means menerapkan pengelompokan fuzzy, dimana setiap data dapat menjadi anggota dari beberapa cluster dengan derajat keanggotaan yang berbeda-beda pada setiap cluster. Fuzzy C-Means merupakan algoritma iteratif, yang menerapkan iterasi pada proses clustering data. Tujuan dari Fuzzy C-Means adalah untuk mendapatkan pusat cluster yang nantinya akan digunakan untuk mengetahui data yang masuk ke dalam sebuah cluster. Untuk informasi lebih lanjut mengenai perhitungan Fuzzy C-Means , silahkan baca disini . Apa itu Silhouette Coefficient ? Silhouette coefficient berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif, sehingga akan menghasilkan nilai silhoutte coeffisien yang maksimum yaitu 1. Maka dapat dikatakan, jika s i = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s i = 0 maka objek i berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s i = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Untuk informasi lebih lanjut mengenai perhitungan silhoutte coeffisien , silahkan baca disini . Apa yang akan dilakukan ? Install library yang digunakan import numpy as np from sklearn.metrics import silhouette_score import skfuzzy as fuzz Library yang digunakan ialah numpy yang digunakan untuk melakukan proses perhitungan, sklearn yang digunakan untuk menghitung nilai silhouette dan skfuzzy yang digunakan untuk melakukan proses perhitungan fuzzy c-means . Fuzzy C-means cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(np.asarray(data).T, i, 2, 0.00001, 1000, seed=0) Untuk menghitung fuzzy c-means telah disediakan oleh library python, adapun inputnya yakni : data berupa array 2d merupakan data yang akan dikelompokkan. jumlah cluster atau kelas yang diinginkan. array eksponensial batas error jumlah maksimum iterasi Sedangkan outputnya berupa : cntr merupakan pusat cluster u merupakan matriks partisi-c fuzzy akhir u0 merupakan tebakan awal pada fuzzy c-partisied matrix distant merupakan matriks jarak euclidian fObj menrupakan riwayat fungsi obyektif iterasi merupakan jumlah iterasi yang dijalankan fpc: merupakan koefisien partisi fuzzy akhir. Yang akan digunakan pada tutorial ini adalah u. Silhouette Coefficient silhouette_score(data, membership, random_state=10) Setelah setiap data telah di cluster , selanjutnya adalah menghitung silhouette score . Silhouette score dihitung pada setiap cluster, baik itu untuk uni-gram maupun bi-gram . Full Code def clustering(data): s_avg = [] for i in range(2,len(data)): cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(np.asarray(data).T, i, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) s_avg.append(silhouette_score(data, membership, random_state=10)) return s_avg Cara akses #Uni Gram tfidf = load_list(\"tf_idf\") s_avg = clustering(tfidf) #Bi Gram tfidf = load_list(\"tf_idf_2\") s_avg = clustering(tfidf) Hasil Setelah diperoleh silhouette score untuk masing-masing cluster pada uni-gram dan bi-gram dengan menggunakan perintah dibawah : #Uni Gram s_avg = func.load_list(\"s_avg\") print(s_avg.index(max(s_avg))+2) #Bi Gram s_avg = func.load_list(\"s_avg_2\") print(s_avg.index(max(s_avg))+2) Diperoleh bahwa cluster yang memiliki silhouette score paling tinggi adalah 2. Conclusion Hasil yang diperoleh pada percobaan di atas mungkin bisa dikatakan tidak terlalu memiliki hasil yang baik, dan mungkin code line yang ada masih terlalu panjang. Meski tutorial ini masih banyak kekurangan, semoga dapat memberikan manfaat untuk teman-teman semua. Jika ada masukan atau perbaikan yang memberikan hasil yang lebih baik atau yang memiliki code program yang lebih pendek, jangan sungkan untuk memberikan saran kalian melalui email. Full Code bisa di download disini , termasuk file excel dan list yang tersimpan. References http://mtkmudahbanar.blogspot.com/2016/08/metode-clustering-k-means-dan-fuzzy-c.html https://lookmylife.wordpress.com/2011/10/03/metode-silhoutte-coeffisien/ https://www.gurupendidikan.co.id/pengertian-data-menurut-para-ahli-serta-jenis-fungsi-dan-contoh/ https://www.jogjawebseo.com/pengertian-apa-itu-web-crawler/ https://wahyudisetiawan.wordpress.com/tag/seleksi-fitur/ https://realpython.com/installing-python/ https://medium.com/@adamaulia/python-simple-crawling-using-beautifulsoup-8247657c2de5 https://agustiyadi.wordpress.com/2013/10/21/pentingnya-sebuah-data/ https://informatikalogi.com/text-preprocessing/ https://informatikalogi.com/term-weighting-tf-idf/ https://yudiagusta.wordpress.com/clustering/ https://datatofish.com/install-package-python-using-pip/ https://temukembaliinformasi.wordpress.com/2009/08/26/pembobotan-tf-idf/ https://arfianhidayat.com/algoritma-tf-idf https://devtrik.com/python/text-preprocessing-dengan-python-nltk/ https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/ https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf","title":"Clustering"},{"location":"clustering/#introduction","text":"Menurut bahasa, data merupakan bentuk jamak dari kata datum (bahasa latin) yang berarti sesuatu yang diberikan . Menurut istilah, pengertian data adalah kumpulan informasi atau keterangan-keterangan yang diperoleh dari pengamatan, informasi itu bisa berupa angka, lambang atau sifat. Dalam kehidupan sehari-hari data berarti suatu pernyataan yang diterima secara apa adanya. Artinya data yang diperoleh dari berbagai sumbernya masih menjadi sebuah anggapan atau fakta karena memang belum diolah lebih lanjut. Setelah diolah melaui suatu penelitian atau percobaan maka data dapat berubah menjadi bentuk yang lebih kompleks misal database, informasi atau bahkan solusi pada masalah tertentu. Data yang telah diolah akan berubah menjadi sebuah informasi yang dapat digunakan untuk menambah pengetahuan bagi yang menerimanya. Maka dalam hal ini data dapat dianggap sebagai obyek dan informasi adalah suatu subyek yang bermanfaat bagi penerimanya. Informasi juga bisa disebut sebagai hasil pengolahan ataupun pemrosesan data. Oleh karena itu artikel ini akan membahas tutorial untuk mengolah data yang diperoleh dari crawling web sehingga diperoleh suatu informasi. Data yang didapat dari proses crawl akan diolah menggunakan metode Fuzzy C-Means untuk melakukan cluster pada data dan Silhouette Coefficient digunakan untuk menguji kualitas cluster yang diperoleh. Hasil akhir yang ingin dicapai pada artikel ini adalah membandingkan jumlah cluster sehingga mendapatkan jumlah cluster yang efiseien dan berkualitas berdasarkan perolehan skor dari Silhouette Coefficient . Selain itu pada tutorial ini juga terdapat proses Feature Selection untuk mengurangi jumlah fitur agar tidak memakan waktu komputasi yang banyak. Algoritma yang digunakan pada proses ini adalah Drop High Correlation , algortima ini digunakan untuk menghindari membuang fitur yang memiliki informasi yang diperlukan karena algoritma ini melakukan proses korelasi matrik untuk mengetahui fitur mana yang tidak memiliki korelasi dengan fitur yang lainnya. Sehingga algoritma ini dapat mencegah membuang fitur yang penting.","title":"Introduction"},{"location":"clustering/#step-1-installing","text":"Dalam tutorial ini semua proses akan dibangun menggunakan bahasa pemograman Python . Alasan mengapa author menggunakan bahasa pemograman ini karena Python merupakan bahasa pemograman yang populer akhir-akhir ini, selain itu Python menyediakan banyak library yang mudah digunakan dalam membangun suatu sistem. Adapun library yang digunakan akan dijelaskan pada setiap stepnya. Panduan instalasi python dapat teman-teman baca disini . Panduan instalasi library atau package dapat teman-teman baca disini .","title":"Step 1. Installing"},{"location":"clustering/#step-2-crawling","text":"","title":"Step 2. Crawling"},{"location":"clustering/#apa-itu-web-crawler","text":"Web Crawler merupakan suatu program atau Script otomatis yang relatif simpel, menggunakan sebuah metode tertentu untuk melakukan scan atau crawl pada halaman internet untuk mendapatkan indek dari data yang dicari. Nama lain dari Web Crawler adalah Web Spider , Web Robot , Bot , Crawl dan Automatic Indexer . Umumnya Web Crawler dapat digunakan berkaitan dengan Search Engine , yakni mengumpulkan informasi mengenai apa yang ada pada halaman-halaman web publik.","title":"Apa itu Web Crawler ?"},{"location":"clustering/#bagaimana-cara-kerja-web-crawler","text":"Ketika Web Crawler suatu Search Engine mengunjungi halaman web, Web Crawler akan membaca teks , hyperlink dan macam-macam tag konten yang digunakan dalam situs misalnya meta tag yang berisi banyak keyword. Data tersebut kemudian akan dimasukkan ke dalam database atau tempat penyimpanan.","title":"Bagaimana cara kerja Web Crawler ?"},{"location":"clustering/#apa-yang-akan-dilakukan","text":"","title":"Apa yang akan dilakukan ?"},{"location":"clustering/#install-library-yang-digunakan","text":"from bs4 import BeautifulSoup import requests BeautifulSoup digunakan untuk dapat mengakses data html dan xml, sedangkan requests digunakan untuk dapat mengakses halaman web.","title":"Install library yang digunakan"},{"location":"clustering/#menentukan-halaman-web-yang-akan-di-crawl","text":"Pada artikel ini halaman web yang akan di crawl datanya adalah https://www.suara.com/indeks/terkini/all/2019 . Halaman web ini memuat artikel atau berita https://www.suara.com yang diupload pada tahun 2019 saja.","title":"Menentukan halaman web yang akan di crawl"},{"location":"clustering/#menentukan-informasi-apa-yang-akan-di-simpan","text":"Setelah menentukan halaman web yang akan di crawl, selanjutnya berdasarkan informasi yang ada di halaman web diatas, data artikel atau berita yang akan di simpan pada tutorial ini berupa URL, judul dan isi berita .","title":"Menentukan informasi apa yang akan di simpan"},{"location":"clustering/#proses-crawling","text":">> Membuat requests pada halaman web yang dituju req = requests.get('https://www.suara.com/indeks/terkini/all/2019') >> Menentukan berita yang akan diambil dari halaman web Berita yang akan kita ambil adalah yang terletak di panel sebelah kiri, berita ini merupakan berita yang diupload pada tahun 2019. Sedangkan berita pada panel kanan merupakan headline setiap minggunya. Untuk dapat mengakses tag html dari list berita tersebut, dapat dilakukan dengan inspect element , yakni dengan cara : klik kanan pada halaman web pilih inspect maka akan muncul panel inspect di jendela sebelah kanan Selanjutnya pada panel inspect , cari tag html yang memuat list berita pada halaman web. Biasanya list berita memiliki nama class yang sama pada setiap berita. Pada halaman web diatas contohnya adalah class item-outer . news_links = soup.find_all(\"li\",{'class':'item-outer'}) >> Mendapatkan judul dan alamat web dari isi artikel Setelah tag html list berita telah diperoleh, selanjutnya adalah mencari tag html yang memuat judul dan link dari judul tersebut. Hal ini dilakukan agar menelusuri isi dari setiap list berita yang terdapat pada halaman tersebut. Biasanya tag html yang memuat link terdapat attribut href . Dari list berita tersebut, jika ditelusuri lebih mendalam tag html yang memuat link berita memiliki class ellipsis2 . for idx,news in enumerate(news_links): title_news= news.find('a',{'class':'ellipsis2'}).text url_news = news.find('a',{'class':'ellipsis2'}).get('href') Dilakukan perulangan untuk membaca semua link yang terdapat pada list berita. >> Membuat requests pada setiap berita req_news = requests.get(url_news) soup_news = BeautifulSoup(req_news.text, \"lxml\") Setelah mendapatkan link setiap berita, sama seperti langkah sebelumnya hal yang dilakukan adalah melakukan requests pada setiap berita. >> Mendapatkan isi artikel pada berita Sama seperti langkah sebelumnya, yang harus dilakukan selanjutnya adalah melakukan Inspect pada salah satu berita kemudiah mancari tag html yang mengandung isi dari berita tersebut. news_content = soup_news.find(\"div\",{'class':'content-article'}) p = news_content.find_all('p') content = ' '.join(item .text for item in p) news_content = content.encode('utf8','replace') Setelah mendapatkan isi dari suatu berita yang terdapat pada tag p selanjutnya menggabungkan isi semua tag tersbut menjadi sebuah string. Note : program diatas tidak dapat digunakan pada halaman web yang lain. Alasannya karena setiap web memiliki tag html yang berbeda. Jadi jika ingin melakukan crawling pada halaman web yang lain, teman-teman harus menyesuaikan dengan tag htmlnya dengan melakukan inspect yang telah dicontohkan diatas.","title":"Proses Crawling"},{"location":"clustering/#menyimpan-data-dalam-bentuk-excel","text":"import openpyxl def save_content(data): woorkbook = create_excel() woorkbook.create_sheet('crawl_data') sheet1 = woorkbook['crawl_data'] i = 0 content = [] for i in range(len(data)): for j in range(len(data[0])): sheet1.cell(row=i+1, column=j+1).value = data[i][j] if(j==1): content.append(data[i][j]) woorkbook.save('excel_logs/'+'crawl_data'+'.xlsx') Libary yang digunakan untuk mengolah data menggunakan excel adalah openpyxl . Library ini sangat mudah untuk mengakses data pada excel, setelah melakukan inisialisasi woorkbook = create_excel() dan mengakses sheet yang akan digunakan sheet1 = woorkbook['crawl_data'] , data dapat disimpan berdasarkan baris dan kolom pada excel sheet1.cell(row=i+1, column=j+1).value = data[i][j] . Setelah semua baris dan kolom sudah diberikan nilai maka selanjutnya adalah menyimpan file excel dengan woorkbook.save('nama.xlsx') .","title":"Menyimpan data dalam bentuk excel"},{"location":"clustering/#menyimpan-data-dalam-bentuk-list","text":"import pickle def save_list(name,l): with open(\"data_logs/\"+name+\".txt\", \"wb\") as fp: #Pickling pickle.dump(l, fp) return print(\"Success to save....\") Data disimpan dalam bentuk list agar diproses selanjutnya data dapat digunakan tanpa melakukan crawling dari awal. Library yang digunakan adalah pickle .","title":"Menyimpan data dalam bentuk list"},{"location":"clustering/#full-code","text":"def crawl_web(url): new_url = url i = '?page=' page = 1 loop = True data = [] while loop == True : #find article link req = requests.get(new_url) soup = BeautifulSoup(req.text, \"lxml\") news_links = soup.find_all(\"li\",{'class':'item-outer'}) #check content available if not news_links : loop = False #looping through article link for idx,news in enumerate(news_links): try : #find news title title_news= news.find('a',{'class':'ellipsis2'}).text #find urll news url_news = news.find('a',{'class':'ellipsis2'}).get('href') #find news content in url req_news = requests.get(url_news) soup_news = BeautifulSoup(req_news.text, \"lxml\") #find news content news_content = soup_news.find(\"div\",{'class':'content-article'}) #find paragraph in news content p = news_content.find_all('p') content = ' '.join(item .text for item in p) news_content = content.encode('utf8','replace') tempt = [] tempt.append(title_news) tempt.append(news_content) tempt.append(url_news) data.append(tempt) except: pass print(page) #next pagination if(page>50): loop = False page+=1 new_url = url+i+str(page) return data","title":"Full Code"},{"location":"clustering/#cara-akses","text":"crawl = crawl_web('https://www.suara.com/indeks/terkini/all/2019') save_content(crawl) save_list(\"crawl\",crawl) List akan disimpan dengan nama crawl di directory data_log sedangkan file excel akan disimpan di directory excel_log . Sehingga sebelum code dijalankan, buatlah kedua folder tersebut terlebih dahulu pada directory yang sama dengan file .py Notes : dalam beberapa kasus akan muncul pesan error lxml saat code dijalankan, jika demikian yang harus dilakukan adalah menginstall library lxml.","title":"Cara akses"},{"location":"clustering/#hasil","text":"Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini .","title":"Hasil"},{"location":"clustering/#step-3-preprocessing","text":"","title":"Step 3. Preprocessing"},{"location":"clustering/#apa-itu-text-preprocessing","text":"Berdasarkan ketidak teraturan struktur data teks, maka proses sistem temu kembali informasi ataupun text mining memerlukan beberapa tahap awal yang pada intinya adalah mempersiapkan agar teks dapat diubah menjadi lebih terstruktur. Salah satu implementasi dari text mining adalah tahap Text Preprocessing. Tahap Text Preprocessing adalah tahapan dimana aplikasi melakukan seleksi data yang akan diproses pada setiap dokumen. Proses preprocessing ini meliputi (1) case folding, (2) tokenizing, (3) filtering, dan (4) stemming.","title":"Apa itu Text Preprocessing ?"},{"location":"clustering/#apa-yang-akan-dilakukan_1","text":"","title":"Apa yang akan dilakukan ?"},{"location":"clustering/#install-library-yang-digunakan_1","text":"import re from nltk.tokenize import word_tokenize from nltk.corpus import stopwords from Sastrawi.Stemmer.StemmerFactory import StemmerFactory Library yang digunakan pada text preprocessing ialah re digunakan untuk symbol removing , nltk digunakan untuk tokenizing dan stopword, Sastrawi digunakan untuk stemming.","title":"Install library yang digunakan"},{"location":"clustering/#symbol-removing","text":"def symbol_remover(data): new_data = [] for i in range(len(data)): new_data.append(re.sub(r'[^\\w]', ' ', data[i].replace(\"b suara com\", \"\"))) return new_data","title":"Symbol Removing"},{"location":"clustering/#case-folding","text":"def lowercase(data): new_data = [] for i in range(len(data)): tempt = str(data[i]).lower() new_data.append(tempt.replace('b\"suara.com - ',\"\")) return new_data Dalam sebuah dokumen penggunan huruf kapital atau sejenisnya terkadang tidak mempunyai kesamaan, hal ini bisa dikarenakan kesalahan penulisan. Dalam text preprocessing proses case folding bertujuan untuk mengubah semua huruf dalam sebuah dokumen teks menjadi huruf kecil (lowercase). Pada data yang didapatkan saat proses crawling , terdapat sebuah kalimat b\"suara.com diawal paragraf pada semua dokumen. Pada program diatas kalimat tersebut di hapus untuk mengurangi banyaknya fitur setiap berita.","title":"Case Folding"},{"location":"clustering/#tokenizing","text":"def tokenisasi(data): new_data = [] for i in range(len(data)): new_data.append(word_tokenize(data[i])) return new_data Dokumen teks terdiri dari sekumpulan kalimat, proses tokenisasi memecah dokumen tersebut menjadi bagian-bagian kata yang disebut token. Contohnya kalimat \"saya menyapu halaman di depan rumah\" setelah ditokenisasi menjadi sebuah list [\"saya\", \"menyapu\", \"halaman\", \"di\", \"depan\", \"rumah\"] .","title":"Tokenizing"},{"location":"clustering/#stopword","text":"def stopword_s(data): new_data = [] stop_words = set(stopwords.words('indonesian')) for i in range(len(data)): tempt = [] for word in data[i]: if word not in stop_words: tempt.append(word) new_data.append(tempt) return new_data Selanjutnya ialah mengambil kata-kata yang dianggap penting dari hasil tokenization atau membuang kata-kata yang dianggap tidak terlalu mempunyai arti penting dalam proses text mining. Contohnya data [\"saya\", \"menyapu\", \"halaman\", \"di\", \"depan\", \"rumah\"] menjadi [\"saya\", \"menyapu\", \"halaman\", \"depan\", \"rumah\"] .","title":"Stopword"},{"location":"clustering/#stemming","text":"def stemming(data): factory = StemmerFactory() stemmer = factory.create_stemmer() new_data = [] for i in range(len(data)): tempt = \"\" for word in data[i]: tempt = tempt + \" \" + word new_word = stemmer.stem(tempt) new_data.append(new_word.split()) return new_data Stemming bertujuan untuk mentransformasikan kata menjadi kata dasarnya (root word) dengan menghilangkan semua imbuhan kata. Contohnya data [\"saya\", \"sapu\", \"halaman\", \"depan\", \"rumah\"] menjadi [\"saya\", \"sapu\", \"halaman\", \"depan\", \"rumah\"] .","title":"Stemming"},{"location":"clustering/#cara-akses_1","text":"Sebelum melakukan proses diatas, terlebih dahulu membaca data hasil crawling yang telah disimpan dalam bentuk list dengan cara dibawah ini. def load_list(name): with open(\"data_logs/\"+name+\".txt\", \"rb\") as fp: # Unpickling b = pickle.load(fp) print(\"Success to load.....\") return b Selanjutnya, pergunakan semua function pada step ini secara bersama. print(\"Load data......\") data = load_list(\"data\") print(\"Lowercase......\") lowercase = lowercase(data) print(\"Remove symbol......\") symbol_remover = symbol_remover(lowercase) print(\"Tokenisasi......\") tokenisasi = tokenisasi(symbol_remover) print(\"Stopword......\") stopword_s = stopword_s(tokenisasi) print(\"Stemming......\") stemming = stemming(stopword_s) Pada hasil akhir dari tutorial ini, kita akan membandingkan uni-gram dan bi-gram. Uni-gram yakni hasil token merupakan tiap satu suku kata, sedangkan bi-gram hasil token menjadi 2 kata. Contoh bi-gram [\"saya sapu\", \"sapu halaman\", \"halaman depan\", \"depan rumah\"] . Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini .","title":"Cara akses"},{"location":"clustering/#step-4-fiture-selection","text":"","title":"Step 4. Fiture Selection"},{"location":"clustering/#apa-itu-fiture-selection","text":"Pemilihan istilah untuk dijadikan indeks merupakan isu yang penting dalam sistem temu-kembali informasi. Selanjunya proses pemilihan istilah ini disebut dengan seleksi fitur (feature selection).","title":"Apa itu Fiture Selection ?"},{"location":"clustering/#mengapa-fiture-selection-penting","text":"Fitur seleksi dapat menyebabkan berkurangnya ukuran indeks sehingga proses retrieval suatu dokumen menjadi lebih cepat sebab jumlah indeks yang dicari menjadi lebih sedikit. Tugas utama seleksi fitur adalah menentukan istilah-istilah yang layak dijadikan term index atau dengan kata lain membuang (menghilangkan) istilah-istilah yang tidak mungkin dijadikan indeks.","title":"Mengapa Fiture Selection penting ?"},{"location":"clustering/#apa-itu-drop-high-correlation","text":"Korelasi adalah istilah statistik yang dalam penggunaan umum mengacu pada seberapa dekat dua variabel untuk memiliki hubungan linier satu sama lain. Fitur dengan korelasi tinggi lebih linear dan karenanya memiliki efek yang hampir sama pada variabel dependen. Jadi, ketika dua fitur memiliki korelasi tinggi, kita dapat menjatuhkan ( drop ) salah satu dari dua fitur tersebut. Penghapusan fitur yang berbeda dari dataset akan memiliki efek yang berbeda pada nilai p untuk dataset. Kami dapat menghapus fitur yang berbeda dan mengukur nilai p di setiap kasus. Nilai-p yang diukur ini dapat digunakan untuk memutuskan apakah akan mempertahankan fitur atau tidak. Untuk informasi lebih lanjut mengenai perhitungan matrik korelasi bisa teman-teman baca disini .","title":"Apa itu Drop High Correlation ?"},{"location":"clustering/#apa-yang-akan-dilakukan_2","text":"","title":"Apa yang akan dilakukan ?"},{"location":"clustering/#install-library-yang-digunakan_2","text":"import pandas as pd import numpy as np Library yang digunakan pada seleksi fitur ialah pandas digunakan untuk membuat data menjadi data frame dan numpy digunakan untuk menjadikan data menjadi matrix dan mengolahnya.","title":"Install library yang digunakan"},{"location":"clustering/#proses-fiture-selection","text":">> Mengubah data menjadi dataframe Data yang berbentuk array dirubah menjadi dataframe , yakni struktur data tabel dua dimensi yang memiliki label. Hal ini akan memudahkan dalam operasi baris dan kolom. df = pd.DataFrame(X) >> Membuat korelasi matriks Dari matriks korelasi ini dapat dilihat bahwa pada bagian diagonal, nilainya pasti = 1 sebab variabel tersebut berkorelasi sempurna positif dengan dirinya sendiri. Nilai korelasi antara pasangan variabel pada set data1 dan data2, menunjukkan nilai dalam range -1 dan 1. Ini disebabkan oleh standarisasi menggunakan standar deviasi sebagai pembagi pada saat perhitungan hasil korelasinya. corr_matrix = df.corr().abs() >> Menentukan fitur yang akan dihapus Setelah mendapatkan matrik kolerasi, selanjutnya menentukan kolom yang memiliki nilai lebih dari 0.95 . Nilai ini fleksibel, bergantung pada batas nilai fitur yang dianggap penting. to_drop = [column for column in upper.columns if any(upper[column] > 0.95)] >> Menghapus fitur Proses selanjutnya ialah menghapus fitur yang dianggap tidak penting berdasarkan nilai batas yang ditentukan pada proses sebelumnya. new_data = df.drop(df.columns[to_drop], axis=1)","title":"Proses Fiture Selection"},{"location":"clustering/#full-code_1","text":"def drop_highly_correlation(X,Y): # Convert feature matrix into DataFrame df = pd.DataFrame(X) # Create correlation matrix corr_matrix = df.corr().abs() # Select upper triangle of correlation matrix upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)) # Find index of feature columns with correlation greater than 0.95 to_drop = [column for column in upper.columns if any(upper[column] > 0.95)] # Drop features new_data = df.drop(df.columns[to_drop], axis=1) header = list(new_data.columns.values) new_collecting_fiture = [] for i in header : new_collecting_fiture.append(Y[i]) return new_data,new_collecting_fiture","title":"Full Code"},{"location":"clustering/#cara-akses_2","text":"collecting_fiture merupakan label dari setiap kolom atau list yang terdiri dari semua kata yang terdapat pada semua berita dimana tidak ada kata yang sama pada list. Panjang collecting_fiture sama dengan panjang kolom vsm . Sedangkan vsm adalah banyaknya kemunculan kata yang ada di collecting_fiture pada setiap berita. Sehingga diperoleh matrik 2-dimensi. def vsm(data,term): for i in range(len(data)): tempt = [] for word in term: tempt.append(data[i].count(word)) new_data.append(tempt) return new_data Sehingga vsm dan collecting_fiture digunakan sebagai input drop_highly_correlation . #Uni Gram vsm = load_list(\"vsm\") collecting_fiture = load_list(\"collecting_fiture\") new_vsm,new_collecting_fiture = drop_highly_correlation(vsm,collecting_fiture) #Bi Gram vsm_n_gram = load_list(\"vsm_2\") collecting_fiture_n_gram = load_list(\"collecting_fiture_2\") new_vsm_n_gram,new_collecting_fiture_n_gram = drop_highly_correlation(vsm_n_gram,collecting_fiture_n_gram)","title":"Cara akses"},{"location":"clustering/#hasil_1","text":"Hasil dari seleksi fitur ini adalah jumlah fitur atau atribut atau kolom pada data, baik uni-gram dan bi-gram akan berkurang. Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini .","title":"Hasil"},{"location":"clustering/#step-5-tf-idf","text":"","title":"Step 5. TF-IDF"},{"location":"clustering/#apa-itu-tf-idf","text":"Metode TF-IDF merupakan metode untuk menghitung bobot setiap kata yang paling umum digunakan pada information retrieval. Metode ini juga terkenal efisien, mudah dan memiliki hasil yang akurat. Metode ini akan menghitung nilai Term Frequency (TF) dan Inverse Document Frequency (IDF) pada setiap token (kata) di setiap dokumen dalam korpus..","title":"Apa itu TF-IDF ?"},{"location":"clustering/#apa-yang-akan-dilakukan_3","text":"","title":"Apa yang akan dilakukan ?"},{"location":"clustering/#install-library-yang-digunakan_3","text":"import math Library yang digunakan ialah math yang digunakan untuk melakukan proses perhitungan.","title":"Install library yang digunakan"},{"location":"clustering/#tf","text":"TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar. def tf(data,term): new_data=[] for i in range(len(data)): tempt = [] for word in term: tempt.append(data[i].count(word)) new_data.append(tempt) return new_data","title":"TF"},{"location":"clustering/#idf","text":"IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah term dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung term yang dimaksud, maka nilai IDF semakin besar. def idf(data): new_data = [] for i in range(len(data[0])): count = 0 for j in range(len(data)): count+= int(data[j][i]) new_data.append(math.log10(len(data)/count)) return new_data","title":"IDF"},{"location":"clustering/#tf-idf","text":"def tf_idf(tf,idf): new_data = [] for i in range(len(tf)): tempt = [] for j in range(len(tf[i])): tempt.append(int(tf[i][j])*int(idf[j])) new_data.append(tempt) return new_data","title":"TF-IDF"},{"location":"clustering/#cara-akses_3","text":"#Uni Gram stemming = load_list(\"stemming\") collecting_fiture = load_list(\"new_collecting_fiture\") print(\"Load Tf....\") tf_a = tf(stemming,collecting_fiture) print(\"Load idf...\") idf_a = idf(tf_a) print(\"Load tf_idf...\") tf_idf_a = tf_idf(tf_a,idf_a,collecting_fiture) #Bi Gram n_gram = load_list(\"token_2\") collecting_fiture_n_gram = load_list(\"new_collecting_fiture_2\") print(\"Load Tf_n_gram....\") tf_n_gram = tf(n_gram,collecting_fiture_n_gram) print(\"Load idf_n_gram...\") idf_n_gram = idf(tf_n_gram) print(\"Load tf_idf_n_gram...\") tf_idf_n_gram = tf_idf(tf_n_gram,idf_n_gram,collecting_fiture_n_gram) Jika teman-teman membutuhkan contoh serupa dengan studi kasus yang berbeda, silahkan baca disini .","title":"Cara akses"},{"location":"clustering/#step-6-clustering","text":"","title":"Step 6. Clustering"},{"location":"clustering/#apa-itu-clustering","text":"Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain.","title":"Apa itu Clustering ?"},{"location":"clustering/#apa-itu-fuzzy-c-means","text":"Fuzzy C-Means menerapkan pengelompokan fuzzy, dimana setiap data dapat menjadi anggota dari beberapa cluster dengan derajat keanggotaan yang berbeda-beda pada setiap cluster. Fuzzy C-Means merupakan algoritma iteratif, yang menerapkan iterasi pada proses clustering data. Tujuan dari Fuzzy C-Means adalah untuk mendapatkan pusat cluster yang nantinya akan digunakan untuk mengetahui data yang masuk ke dalam sebuah cluster. Untuk informasi lebih lanjut mengenai perhitungan Fuzzy C-Means , silahkan baca disini .","title":"Apa itu Fuzzy C-Means ?"},{"location":"clustering/#apa-itu-silhouette-coefficient","text":"Silhouette coefficient berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif, sehingga akan menghasilkan nilai silhoutte coeffisien yang maksimum yaitu 1. Maka dapat dikatakan, jika s i = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s i = 0 maka objek i berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s i = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Untuk informasi lebih lanjut mengenai perhitungan silhoutte coeffisien , silahkan baca disini .","title":"Apa itu Silhouette Coefficient ?"},{"location":"clustering/#apa-yang-akan-dilakukan_4","text":"","title":"Apa yang akan dilakukan ?"},{"location":"clustering/#install-library-yang-digunakan_4","text":"import numpy as np from sklearn.metrics import silhouette_score import skfuzzy as fuzz Library yang digunakan ialah numpy yang digunakan untuk melakukan proses perhitungan, sklearn yang digunakan untuk menghitung nilai silhouette dan skfuzzy yang digunakan untuk melakukan proses perhitungan fuzzy c-means .","title":"Install library yang digunakan"},{"location":"clustering/#fuzzy-c-means","text":"cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(np.asarray(data).T, i, 2, 0.00001, 1000, seed=0) Untuk menghitung fuzzy c-means telah disediakan oleh library python, adapun inputnya yakni : data berupa array 2d merupakan data yang akan dikelompokkan. jumlah cluster atau kelas yang diinginkan. array eksponensial batas error jumlah maksimum iterasi Sedangkan outputnya berupa : cntr merupakan pusat cluster u merupakan matriks partisi-c fuzzy akhir u0 merupakan tebakan awal pada fuzzy c-partisied matrix distant merupakan matriks jarak euclidian fObj menrupakan riwayat fungsi obyektif iterasi merupakan jumlah iterasi yang dijalankan fpc: merupakan koefisien partisi fuzzy akhir. Yang akan digunakan pada tutorial ini adalah u.","title":"Fuzzy C-means"},{"location":"clustering/#silhouette-coefficient","text":"silhouette_score(data, membership, random_state=10) Setelah setiap data telah di cluster , selanjutnya adalah menghitung silhouette score . Silhouette score dihitung pada setiap cluster, baik itu untuk uni-gram maupun bi-gram .","title":"Silhouette Coefficient"},{"location":"clustering/#full-code_2","text":"def clustering(data): s_avg = [] for i in range(2,len(data)): cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(np.asarray(data).T, i, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) s_avg.append(silhouette_score(data, membership, random_state=10)) return s_avg","title":"Full Code"},{"location":"clustering/#cara-akses_4","text":"#Uni Gram tfidf = load_list(\"tf_idf\") s_avg = clustering(tfidf) #Bi Gram tfidf = load_list(\"tf_idf_2\") s_avg = clustering(tfidf)","title":"Cara akses"},{"location":"clustering/#hasil_2","text":"Setelah diperoleh silhouette score untuk masing-masing cluster pada uni-gram dan bi-gram dengan menggunakan perintah dibawah : #Uni Gram s_avg = func.load_list(\"s_avg\") print(s_avg.index(max(s_avg))+2) #Bi Gram s_avg = func.load_list(\"s_avg_2\") print(s_avg.index(max(s_avg))+2) Diperoleh bahwa cluster yang memiliki silhouette score paling tinggi adalah 2.","title":"Hasil"},{"location":"clustering/#conclusion","text":"Hasil yang diperoleh pada percobaan di atas mungkin bisa dikatakan tidak terlalu memiliki hasil yang baik, dan mungkin code line yang ada masih terlalu panjang. Meski tutorial ini masih banyak kekurangan, semoga dapat memberikan manfaat untuk teman-teman semua. Jika ada masukan atau perbaikan yang memberikan hasil yang lebih baik atau yang memiliki code program yang lebih pendek, jangan sungkan untuk memberikan saran kalian melalui email. Full Code bisa di download disini , termasuk file excel dan list yang tersimpan.","title":"Conclusion"},{"location":"clustering/#references","text":"http://mtkmudahbanar.blogspot.com/2016/08/metode-clustering-k-means-dan-fuzzy-c.html https://lookmylife.wordpress.com/2011/10/03/metode-silhoutte-coeffisien/ https://www.gurupendidikan.co.id/pengertian-data-menurut-para-ahli-serta-jenis-fungsi-dan-contoh/ https://www.jogjawebseo.com/pengertian-apa-itu-web-crawler/ https://wahyudisetiawan.wordpress.com/tag/seleksi-fitur/ https://realpython.com/installing-python/ https://medium.com/@adamaulia/python-simple-crawling-using-beautifulsoup-8247657c2de5 https://agustiyadi.wordpress.com/2013/10/21/pentingnya-sebuah-data/ https://informatikalogi.com/text-preprocessing/ https://informatikalogi.com/term-weighting-tf-idf/ https://yudiagusta.wordpress.com/clustering/ https://datatofish.com/install-package-python-using-pip/ https://temukembaliinformasi.wordpress.com/2009/08/26/pembobotan-tf-idf/ https://arfianhidayat.com/algoritma-tf-idf https://devtrik.com/python/text-preprocessing-dengan-python-nltk/ https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/ https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf","title":"References"}]}